 
**Team members**:

Shilpa M - Monday batch (online)

Sushmitha M Katti - Friday batch (online)
<br/><br/>

**Parameters**<br/><br/>
Total params: 13,112<br/>
Trainable params: 12,912<br/>
Non-trainable params: 200<br/><br/>

**Logs Of 20 Epochs**<br/><br/>
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 13s 223us/step - loss: 0.5403 - acc: 0.8459 - val_loss: 0.1010 - val_acc: 0.9833
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 10s 173us/step - loss: 0.2519 - acc: 0.9255 - val_loss: 0.0541 - val_acc: 0.9896
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 11s 177us/step - loss: 0.1979 - acc: 0.9406 - val_loss: 0.0542 - val_acc: 0.9876
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 11s 177us/step - loss: 0.1710 - acc: 0.9449 - val_loss: 0.0362 - val_acc: 0.9910
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 11s 177us/step - loss: 0.1497 - acc: 0.9506 - val_loss: 0.0300 - val_acc: 0.9929
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 11s 176us/step - loss: 0.1375 - acc: 0.9520 - val_loss: 0.0268 - val_acc: 0.9922
Epoch 7/20
Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 11s 176us/step - loss: 0.1306 - acc: 0.9522 - val_loss: 0.0292 - val_acc: 0.9922
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 11s 176us/step - loss: 0.1226 - acc: 0.9544 - val_loss: 0.0241 - val_acc: 0.9940
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 11s 175us/step - loss: 0.1171 - acc: 0.9546 - val_loss: 0.0229 - val_acc: 0.9934
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 11s 177us/step - loss: 0.1111 - acc: 0.9564 - val_loss: 0.0213 - val_acc: 0.9941
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 11s 176us/step - loss: 0.1096 - acc: 0.9559 - val_loss: 0.0236 - val_acc: 0.9932
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 11s 176us/step - loss: 0.1058 - acc: 0.9561 - val_loss: 0.0203 - val_acc: 0.9943
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 11s 177us/step - loss: 0.1043 - acc: 0.9558 - val_loss: 0.0214 - val_acc: 0.9944
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 11s 187us/step - loss: 0.1035 - acc: 0.9554 - val_loss: 0.0189 - val_acc: 0.9947
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 11s 179us/step - loss: 0.1011 - acc: 0.9564 - val_loss: 0.0194 - val_acc: 0.9946
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 11s 180us/step - loss: 0.0969 - acc: 0.9562 - val_loss: 0.0192 - val_acc: 0.9942
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 11s 179us/step - loss: 0.0958 - acc: 0.9572 - val_loss: 0.0184 - val_acc: 0.9944
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 11s 178us/step - loss: 0.0969 - acc: 0.9575 - val_loss: 0.0185 - val_acc: 0.9948
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 11s 178us/step - loss: 0.0928 - acc: 0.9576 - val_loss: 0.0206 - val_acc: 0.9942
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 11s 177us/step - loss: 0.0945 - acc: 0.9582 - val_loss: 0.0177 - val_acc: 0.9944


**Score**
[0.017742081358947327, 0.9944]

**Procedure**<br/><br/>
1. Ran all the eight codes
2. Took 8th code because 8th code was perfect. It was giving >99.4% test accuracy. Only problem was number of parameters was >15k
3. So reduced number of kernels in first 2 layers to 10, 16 instead of 16,32.
4. Got the conditions satisfied.(99.4% accuracy, <15k Parameters, within 20 Ephocs)

