 
**Team members**:

Shilpa M - Monday batch (online)

Sushmitha M Katti - Friday batch (online)
<br/><br/>

**Parameters**<br/><br/>
Total params: 12,956<br/>
Trainable params: 12,776<br/>
Non-trainable params: 180<br/><br/>

**Logs Of 20 Epochs**<br/><br/>
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 6s 107us/step - loss: 0.2106 - acc: 0.9334 - val_loss: 0.0610 - val_acc: 0.9806
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 5s 81us/step - loss: 0.0703 - acc: 0.9786 - val_loss: 0.0435 - val_acc: 0.9852
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 5s 81us/step - loss: 0.0546 - acc: 0.9828 - val_loss: 0.0396 - val_acc: 0.9874
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0465 - acc: 0.9858 - val_loss: 0.0430 - val_acc: 0.9862
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 5s 83us/step - loss: 0.0419 - acc: 0.9865 - val_loss: 0.0308 - val_acc: 0.9893
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 5s 83us/step - loss: 0.0377 - acc: 0.9879 - val_loss: 0.0226 - val_acc: 0.9923
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0335 - acc: 0.9899 - val_loss: 0.0240 - val_acc: 0.9918
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0310 - acc: 0.9898 - val_loss: 0.0233 - val_acc: 0.9923
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0300 - acc: 0.9904 - val_loss: 0.0288 - val_acc: 0.9904
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0282 - acc: 0.9905 - val_loss: 0.0211 - val_acc: 0.9926
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 5s 81us/step - loss: 0.0269 - acc: 0.9911 - val_loss: 0.0199 - val_acc: 0.9930
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0262 - acc: 0.9917 - val_loss: 0.0193 - val_acc: 0.9934
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0196 - val_acc: 0.9932
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0232 - acc: 0.9924 - val_loss: 0.0182 - val_acc: 0.9933
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 5s 81us/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0187 - val_acc: 0.9940
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 5s 85us/step - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0186 - val_acc: 0.9946
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 5s 86us/step - loss: 0.0218 - acc: 0.9929 - val_loss: 0.0169 - val_acc: 0.9946
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 5s 83us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0160 - val_acc: 0.9942
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 5s 82us/step - loss: 0.0190 - acc: 0.9934 - val_loss: 0.0177 - val_acc: 0.9943
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 5s 81us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0178 - val_acc: 0.9940




**Score**
[0.017836555069088354, 0.994]

**Procedure**<br/>
1. Ran all the eight codes.Built an intution around it.
2. Took 8th code because 8th code was perfect. It was giving >99.4% test accuracy. Only problem was number of parameters was >15k and dropout, batch normalisation was added to last layer as well
3. So reduced number of kernels in first 2 layers to 16, 10 instead of 16,32.And removed the batch normalisation and ddropout.And set **use_bias = False**
4. Got the conditions satisfied.(>99.4% accuracy, <15k Parameters, within 20 Ephocs)

